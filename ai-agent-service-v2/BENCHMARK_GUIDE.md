# üöÄ Claude Model Benchmark Guide

**Claude Haiku 4.5 vs Claude Sonnet 4.5 Comparison**

## Overview

This benchmark compares two Claude models across different dimensions:
- **Response Quality** - Task completion success rate
- **Performance** - Response latency and throughput
- **Cost Efficiency** - Token usage and pricing
- **Cache Effectiveness** - Prompt caching benefits

## Quick Start

### 1. Install Dependencies

```bash
cd ai-agent-service-v2
pip install -r requirements.txt
pip install pyyaml  # For loading test scenarios
```

### 2. Set Up Environment Variables

Create or update `.env`:
```bash
CLAUDE_API_KEY=sk-ant-...  # Your Anthropic API key
BACKEND_API_URL=http://localhost:8014/api/v1
DEFAULT_SHOP_ID=8
```

### 3. Start Required Services

**Terminal 1: Backend API**
```bash
cd ../backend
python main.py  # Runs on port 8014
```

**Terminal 2: MCP Server**
```bash
cd ../mcp-server
./start.sh  # Runs on port 8000
```

### 4. Run the Benchmark

```bash
cd ai-agent-service-v2
python benchmark_models.py
```

## Output

The benchmark generates two files:

### 1. `benchmark_results.json`
Detailed metrics in JSON format:
```json
{
  "haiku": {
    "model": "claude-haiku-4-5-20251001",
    "scenarios_run": 10,
    "scenarios_passed": 9,
    "total_tokens": 45230,
    "total_cost": 0.0234,
    "avg_response_time": 1.23,
    "cache_hit_rate": 82.3,
    ...
  },
  "sonnet": {
    "model": "claude-sonnet-4-5-20250929",
    "scenarios_run": 10,
    "scenarios_passed": 10,
    "total_tokens": 52100,
    "total_cost": 0.1234,
    "avg_response_time": 2.15,
    "cache_hit_rate": 85.1,
    ...
  }
}
```

### 2. `benchmark_results_report.txt`
Human-readable comparison report with recommendations.

## Key Metrics

### üìä Test Results
- **Pass Rate**: Percentage of successful test scenarios
- **Scenarios Run**: Number of YAML test scenarios executed

### ‚ö° Performance
- **Avg Response Time**: Average latency per request
- **Min/Max Response Time**: Performance range
- **Throughput**: Estimated requests per second

### üí∞ Tokens & Costs

**Claude Haiku 4.5 Pricing:**
- Input: $0.80 per 1M tokens
- Output: $4.00 per 1M tokens
- Cache Read: $0.08 per 1M (90% discount)
- Cache Write: $1.00 per 1M (25% premium)

**Claude Sonnet 4.5 Pricing:**
- Input: $3.00 per 1M tokens
- Output: $15.00 per 1M tokens
- Cache Read: $0.30 per 1M (90% discount)
- Cache Write: $3.75 per 1M (25% premium)

### üì¶ Cache Effectiveness
- **Cache Hit Rate**: Percentage of requests hitting cached blocks
- **Tokens Saved**: Total tokens saved by caching

## Test Scenarios

The benchmark uses YAML scenarios from `testing-framework/scenarios/`:

```yaml
name: "–£—Å–ø–µ—à–Ω–æ–µ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ –∑–∞–∫–∞–∑–∞"
description: "Full order creation flow"
initial_message: "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –•–æ—á—É –∑–∞–∫–∞–∑–∞—Ç—å –±—É–∫–µ—Ç —Ä–æ–∑..."

success_criteria:
  products_shown: true
  order_created: true
  goal_achieved: true
```

Current scenarios include:
- ‚úÖ Budget customer queries
- ‚úÖ Regular order creation
- ‚úÖ Order tracking
- ‚úÖ Delivery validation
- ‚úÖ Payment operations
- ‚úÖ VIP customer requests
- ‚úÖ Complex scenarios (multi-turn conversations)
- ‚úÖ Kaspi Pay integration
- ‚úÖ Order updates and cancellations

## Interpreting Results

### When to Use Haiku 4.5 ‚úÖ

```
‚úÖ Use if:
  ‚Ä¢ Success rate ‚â• Sonnet's rate
  ‚Ä¢ Cost < 50% of Sonnet
  ‚Ä¢ Response time < 2s
  ‚Ä¢ Cache hit rate > 70%

üéØ Perfect for:
  ‚Ä¢ Product browsing queries
  ‚Ä¢ Simple order tracking
  ‚Ä¢ FAQ & help requests
  ‚Ä¢ High-volume, simple tasks
```

### When to Use Sonnet 4.5 ‚úÖ

```
‚úÖ Use if:
  ‚Ä¢ Success rate > Haiku's rate
  ‚Ä¢ Complex reasoning required
  ‚Ä¢ Multi-step operations
  ‚Ä¢ Priority on quality > cost

üéØ Perfect for:
  ‚Ä¢ Complex VIP requests
  ‚Ä¢ Multi-turn conversations
  ‚Ä¢ Kaspi Pay operations
  ‚Ä¢ Order modifications
```

### Hybrid Strategy üîÑ

```
Recommended approach:
1. Use Haiku 4.5 as default (fast + cheap)
2. Detect complexity ‚Üí Fall back to Sonnet 4.5 if needed
3. Monitor success rates in production
4. Adjust model selection based on real metrics
```

## Customizing the Benchmark

### Add More Test Scenarios

1. Create YAML file in `testing-framework/scenarios/`:
```yaml
name: "My Custom Test"
description: "Tests specific functionality"
initial_message: "User's question or request"
success_criteria:
  some_criteria: true
  another_criteria: false
```

2. Benchmark will automatically pick it up

### Modify Benchmark Parameters

Edit `benchmark_models.py`:

```python
# Change models tested
models = [
    "claude-haiku-4-5-20251001",
    "claude-sonnet-4-5-20250929",
    # Add more here
]

# Limit scenarios (for faster testing)
yaml_files = sorted(list(scenarios_path.glob("*.yaml")))[:5]  # Only first 5
```

### Run Specific Model Only

```python
# In main():
models = ["claude-haiku-4-5-20251001"]  # Only Haiku
# or
models = ["claude-sonnet-4-5-20250929"]  # Only Sonnet
```

## Troubleshooting

### Backend API Not Available
```
‚ùå Error: Connection refused on port 8014
‚úÖ Solution: Start backend first (./scripts/start-backend.sh)
```

### MCP Server Not Running
```
‚ùå Error: MCP tools failing
‚úÖ Solution: Start MCP server (mcp-server/start.sh)
```

### Out of Memory
```
‚ùå Error: sqlite3.OperationalError: database is locked
‚úÖ Solution:
  rm benchmark.db
  python benchmark_models.py
```

### CLAUDE_API_KEY Not Set
```
‚ùå Error: CLAUDE_API_KEY not set
‚úÖ Solution:
  export CLAUDE_API_KEY=sk-ant-...
  python benchmark_models.py
```

### Scenarios Not Loading
```
‚ùå Error: No test scenarios loaded
‚úÖ Solution:
  ‚Ä¢ Check testing-framework/scenarios/ exists
  ‚Ä¢ Verify YAML files are valid
  ‚Ä¢ Check file permissions
```

## Performance Optimization Tips

### For Faster Benchmarking:
1. Reduce scenarios: `[:5]` instead of `[:10]`
2. Disable database persistence: Use in-memory SQLite
3. Run in parallel batches (advanced)

### For More Accurate Results:
1. Run multiple times (5-10 iterations)
2. Use real user data scenarios
3. Test during stable network conditions
4. Warm up cache first

## Results Interpretation Examples

### Example 1: Haiku is Better ‚≠ê

```
Pass Rate:          90% (9/10)              92% (10/10)
Avg Response Time:  1.2s ‚ö°                1.9s
Cost per Request:   $0.0023                $0.0123
Cache Hit Rate:     82%                    85%

‚úÖ RECOMMENDATION: Use Haiku 4.5
   ‚Ä¢ Similar quality (only 2% difference)
   ‚Ä¢ 5x faster response time
   ‚Ä¢ 5x cheaper
   ‚Ä¢ Excellent cache hit rate
```

### Example 2: Sonnet is Worth It ‚≠ê

```
Pass Rate:          80% (8/10)              95% (10/10) ‚úÖ
Avg Response Time:  1.8s                   2.1s
Cost per Request:   $0.0021                $0.0115
Cache Hit Rate:     75%                    88%

‚ö†Ô∏è RECOMMENDATION: Consider Sonnet 4.5
   ‚Ä¢ 15% higher success rate (important!)
   ‚Ä¢ 5x increase in cost
   ‚Ä¢ Only 0.3s slower
   ‚Ä¢ Better cache efficiency
   ‚Ä¢ Worth it if quality is critical
```

## Next Steps

1. **Run Benchmark**: `python benchmark_models.py`
2. **Review Report**: Check `benchmark_results_report.txt`
3. **Update main.py**: Adjust model selection based on results
4. **Monitor Production**: Track actual success rates with chosen model
5. **Re-benchmark**: Run again after major changes

## References

- [Claude Models Documentation](https://docs.anthropic.com/en/docs/about-claude/models-overview)
- [Prompt Caching Guide](https://docs.anthropic.com/en/docs/build-a-system-with-claude/prompt-caching)
- [API Pricing](https://www.anthropic.com/pricing)

---

**Happy Benchmarking! üöÄ**
